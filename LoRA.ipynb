{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM, GenerationConfig\n",
    "from datasets import load_dataset\n",
    "from peft import PeftModel, PeftConfig, LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForSeq2SeqLMWithValueHead, create_reference_model\n",
    "from trl.core import LengthSampler\n",
    "\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import evaluate\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name=\"google/flan-t5-base\"\n",
    "dataset_name = \"knkarthick/dialogsum\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sample):\n",
    "    sample[\"query\"] = \"summarize: \" + sample[\"dialogue\"]\n",
    "    sample[\"input_ids\"] = tokenizer.encode(sample[\"query\"], return_tensors=\"pt\", padding=\"max_length\").squeeze()\n",
    "    sample[\"labels\"] = tokenizer.encode(sample[\"summary\"], return_tensors=\"pt\", padding=\"max_length\").squeeze()\n",
    "    return sample\n",
    "\n",
    "def build_dataset(dataset_name, split, max_length=512):\n",
    "    dataset = load_dataset(dataset_name, split=split)\n",
    "    dataset = dataset.map(tokenize)\n",
    "    dataset = dataset.filter(lambda x: len(x[\"input_ids\"]) <= max_length)\n",
    "    return dataset\n",
    "\n",
    "train_dataset = build_dataset(dataset_name=dataset_name, split=\"train\")\n",
    "test_dataset = build_dataset(dataset_name=dataset_name, split=\"test\")\n",
    "val_dataset = build_dataset(dataset_name=dataset_name, split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    num_trainable_params = sum(param.numel() for param in model.parameters() if param.requires_grad)\n",
    "    num_all_params = sum(param.numel() for param in model.parameters())\n",
    "    percentage_trainable_params = 100 * num_trainable_params / num_all_params if num_all_params != 0 else 0\n",
    "    return f\"Trainable parameters: {num_trainable_params}\\nAll parameters: {num_all_params}\\nPercentage of trainable parameters: {percentage_trainable_params:.2f}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    #target_modules=[\"wi_0\", \"wi_1\", \"wo\"],\n",
    "    #target_modules=[\"q\", \"k\", \"v\", \"o\"],\n",
    "    target_modules=[\"q\", \"k\", \"v\", \"o\", \"wi_0\", \"wi_1\", \"wo\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "lora_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "lora_model = get_peft_model(lora_model, lora_config)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "\n",
    "print(f'Original model:\\n{count_parameters(model)}\\n')\n",
    "print(f'LoRA model:\\n{count_parameters(lora_model)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"LoRA_E1_W002_BOTH_4\",\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.002,\n",
    "    logging_steps=100,\n",
    "    report_to=None,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot loss curve\n",
    "train_losses = [entry['loss'] for entry in trainer.state.log_history[:-1]]\n",
    "plt.plot([x*training_args.logging_steps for x in range(1, len(train_losses) + 1)], train_losses, label=\"Training Loss\")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prediction(model, sample):\n",
    "    input_ids = torch.tensor([sample[\"input_ids\"]]).to(device)\n",
    "    output = model.generate(input_ids=input_ids)\n",
    "    predicted_summary = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return predicted_summary\n",
    "\n",
    "def compute_rouge(model, dataset):\n",
    "    references = []\n",
    "    predictions = []\n",
    "    for sample in tqdm(dataset):\n",
    "        reference_summary = sample[\"summary\"]\n",
    "        predicted_summary = generate_prediction(model, sample)\n",
    "        references.append(reference_summary)\n",
    "        predictions.append(predicted_summary)\n",
    "    scores = rouge.compute(predictions=predictions, references=references)\n",
    "    return scores\n",
    "\n",
    "#compute rouge\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "scores = compute_rouge(lora_model, test_dataset)\n",
    "print(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
